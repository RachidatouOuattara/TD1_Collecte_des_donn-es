{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD : Collecte des Données en Python pour le\n",
    "Traitement du Langage Naturel (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Objectifs du TD\n",
    "\n",
    "— Comprendre les méthodes de collecte de données pour le NLP.\n",
    "\n",
    "— Manipuler des outils pour extraire des textes à partir de sources variées.\n",
    "\n",
    "\n",
    "— Explorer les sources de données en ligne et apprendre à extraire des informations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 3.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-2.2.3-cp39-cp39-macosx_11_0_arm64.whl (11.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.3 MB 19.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "\u001b[K     |████████████████████████████████| 164 kB 32.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 32.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.0-cp39-cp39-macosx_11_0_arm64.whl (120 kB)\n",
      "\u001b[K     |████████████████████████████████| 120 kB 53.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 34.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[K     |████████████████████████████████| 346 kB 34.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "\u001b[K     |████████████████████████████████| 508 kB 47.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /Users/verdianeouattara/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/verdianeouattara/Library/Python/3.9/lib/python/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Installing collected packages: urllib3, tzdata, soupsieve, pytz, idna, charset-normalizer, certifi, requests, pandas, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.12.3 certifi-2024.12.14 charset-normalizer-3.4.0 idna-3.10 pandas-2.2.3 pytz-2024.2 requests-2.32.3 soupsieve-2.6 tzdata-2024.2 urllib3-2.2.3\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/verdianeouattara/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import des bibliothèques nécessaires\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan du TD\n",
    "1. Introduction et mise en place\n",
    "2. Extraction de données à partir d’une API\n",
    "3. Web scraping de textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L’objectif de ce TD est de collecter des données textuelles à partir de différentes\n",
    "sources et de les préparer pour des applications NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Réponses aux questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "—-->  Quelles sont les principales sources de données textuelles pour le NLP?\n",
    "\n",
    "Corpus existants : Bases linguistiques (ex. : Penn Treebank).\n",
    "\n",
    "Sites web : Articles, blogs, forums via web scraping.\n",
    "\n",
    "API : Services comme Twitter, Reddit, Wikipédia.\n",
    "Médias sociaux : Données textuelles de plateformes sociales.\n",
    "\n",
    "Documents publics : Livres, articles, documents officiels.\n",
    "\n",
    "Données propriétaires : Transcriptions, emails internes.\n",
    "\n",
    "—-->  Pourquoi la collecte de données est-elle essentielle pour le NLP?\n",
    "\n",
    "Permet de créer et d’entraîner des modèles NLP.\n",
    "\n",
    "Améliore la qualité et la précision des algorithmes.\n",
    "\n",
    "Reflète la diversité linguistique et contextuelle.\n",
    "\n",
    "Favorise l’adaptation à des domaines spécifiques.\n",
    "\n",
    "Fournit des données récentes pour des modèles à jour.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Extraction de données à partir d’une API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
