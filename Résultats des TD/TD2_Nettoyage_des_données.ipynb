{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD : Nettoyage des Données Textuelles en Python pour le NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectifs du TD :\n",
    "\n",
    "— Comprendre l’importance du nettoyage des données textuelles pour le NLP.\n",
    "\n",
    "— Manipuler des bibliothèques Python pour normaliser et nettoyer des textes.\n",
    "\n",
    "— Préparer les données collectées pour des applications de traitement automatique du langage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Prérequis :\n",
    "\n",
    "— Données textuelles collectées via une API ou du web scraping (par exemple, à\n",
    "partir du premier TD).\n",
    "\n",
    "— Python 3.7+\n",
    "\n",
    "— Bibliothèques nécessaires : NLTK, Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan du TD :\n",
    "\n",
    "1. Introduction au nettoyage des données\n",
    "\n",
    "2. Nettoyage des textes : suppression des caractères inutiles\n",
    "\n",
    "3. Tokenisation et normalisation\n",
    "\n",
    "4. Sauvegarde des données nettoyées\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1 Introduction au nettoyage des données\n",
    "\n",
    "Les données brutes collectées contiennent souvent du bruit (caractères spéciaux, liens, mentions inutiles) qui doit être nettoyé avant leur utilisation dans des modèles NLP.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Réponses aux questions: \n",
    "\n",
    "### 1. Pourquoi est-il important de nettoyer les données textuelles pour le NLP ?\n",
    "\n",
    "   Le nettoyage de texte est utile, car il permet d'uniformiser les tokens similaires, réduisant ainsi la taille du vocabulaire\n",
    "\n",
    "### 2. Quels sont les principaux types de bruit dans les textes collectés ?\n",
    "\n",
    "  Les principaux types de bruit dans les textes collectés sont :\n",
    "\n",
    " -Données inutiles ou hors sujet : Textes publicitaires, menus de navigation, ou informations redondantes (ex. : copyright, liens).\n",
    "\n",
    "-Caractères spéciaux et ponctuation excessive : Symboles non pertinents (#, *, &, etc.) ou ponctuation répétée.\n",
    "\n",
    "- Balises HTML ou contenu non nettoyé : Balises ou scripts restants après l'extraction (ex. : <div>, <script>).\n",
    "\n",
    "- Erreurs typographiques et grammaticales : Fautes de frappe ou mauvaise syntaxe, surtout dans les contributions générées par des utilisateurs.\n",
    "\n",
    "- Langue mixte ou ambiguïté linguistique : Mélange de langues ou utilisation de mots ayant plusieurs significations\n",
    "    \n",
    "-  Contenu dupliqué : Répétitions fréquentes d'informations similaires ou identiques.\n",
    "\n",
    "- Données manquantes ou incohérentes : Lignes vides, phrases incomplètes, ou incohérence dans le formatage.\n",
    "   \n",
    "-  Ces bruits nécessitent un nettoyage et une prétraitement pour garantir la qualité des données avant toute analyse NLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Nettoyage des textes : suppression des caractères inutiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Étapes à suivre :\n",
    "\n",
    "— Identifier et supprimer les caractères spéciaux et hyperliens.\n",
    "\n",
    "— Convertir les textes en minuscules.\n",
    "\n",
    "— Supprimer les espaces inutiles et normaliser le format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple de comparaison avant/après nettoyage :\n",
      "Texte brut : The Role of Telomerase in Breast Cancer's Response to Therapy - PubMed\n",
      "Texte nettoyé : the role of telomerase in breast cancers response to therapy pubmed\n",
      "\n",
      "Texte brut : This site needs JavaScript to work properly. Please enable it to take advantage of the complete set of features!\n",
      "Texte nettoyé : this site needs javascript to work properly please enable it to take advantage of the complete set of features\n",
      "\n",
      "Texte brut : Clipboard, Search History, and several other advanced features are temporarily unavailable.\n",
      "Texte nettoyé : clipboard search history and several other advanced features are temporarily unavailable\n",
      "\n",
      "Texte brut : Skip to main page content\n",
      "Texte nettoyé : skip to main page content\n",
      "\n",
      "Texte brut : An official website of the United States government\n",
      "Texte nettoyé : an official website of the united states government\n",
      "\n",
      "Les textes nettoyés ont été sauvegardés dans 'contenu_textuel_nettoye.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Charger le fichier CSV\n",
    "input_file = 'contenu_textuel_extrait.csv'  \n",
    "output_file = 'contenu_textuel_nettoye.csv' \n",
    "\n",
    "# Chargement des données\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Fonction de nettoyage des textes\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Suppression des hyperliens\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Supprimer les URLs\n",
    "        # Suppression des caractères spéciaux\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Garder uniquement les lettres, chiffres, et espaces\n",
    "        # Convertion  en minuscules\n",
    "        text = text.lower()\n",
    "        # Suppression des espaces inutiles\n",
    "        text = text.strip() # Supprimer les espaces au début et à la fin\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Remplacement des espaces multiples par un seul\n",
    "    return text\n",
    "\n",
    "# Appliquer le nettoyage\n",
    "df['contenu_textuel_nettoye'] = df['contenu_textuel_extrait'].apply(clean_text)\n",
    "\n",
    "# Comparer quelques exemples avant et après nettoyage\n",
    "print(\"Exemple de comparaison avant/après nettoyage :\")\n",
    "for _, row in df.head(5).iterrows():\n",
    "    print(f\"Texte brut : {row['contenu_textuel_extrait']}\")\n",
    "    print(f\"Texte nettoyé : {row['contenu_textuel_nettoye']}\\n\")\n",
    "\n",
    "# Sauvegarder les résultats dans un fichier CSV dans le répertoire courant\n",
    "df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "print(f\"Les textes nettoyés ont été sauvegardés dans '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des résultats :\n",
    "\n",
    "### -  Comparez les textes bruts aux textes nettoyés.\n",
    "Les textes bruts contiennent des éléments non pertinents comme des caractères spéciaux, des références à des plateformes (PubMed, JavaScript) et des hyperliens inutilisables. Après nettoyage, les majuscules sont converties en minuscules, les caractères spéciaux et hyperliens supprimés, et les espaces superflus éliminés, offrant un format normalisé.\n",
    "\n",
    "\n",
    "### - Identifiez les limites potentielles du nettoyage effectué.\n",
    "Le nettoyage peut entraîner une perte d'informations utiles, comme les apostrophes ou des noms significatifs tels que \"PubMed\". Les mots communs inutiles (stopwords) restent présents, et aucune correction grammaticale n'est effectuée. Les textes génériques ou hors sujet ne sont pas filtrés, et une suppression excessive d'éléments peut rendre les phrases incohérentes ou incomplètes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokénisatiion et normalisation des textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étapes à suivre :\n",
    "\n",
    "— Divisez les textes en unités lexicales (tokens) à l’aide d’une bibliothèque comme NLTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /Users/verdianeouattara/Library/Python/3.9/lib/python/site-packages (3.9.1)\n",
      "Requirement already satisfied: tqdm in /Users/verdianeouattara/Library/Python/3.9/lib/python/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/verdianeouattara/Library/Python/3.9/lib/python/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: click in /Users/verdianeouattara/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/verdianeouattara/Library/Python/3.9/lib/python/site-packages (from nltk) (1.4.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/verdianeouattara/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple de tokenisation :\n",
      "                             contenu_textuel_nettoye  \\\n",
      "0  the role of telomerase in breast cancers respo...   \n",
      "1  this site needs javascript to work properly pl...   \n",
      "2  clipboard search history and several other adv...   \n",
      "3                          skip to main page content   \n",
      "4  an official website of the united states gover...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [the, role, of, telomerase, in, breast, cancer...  \n",
      "1  [this, site, needs, javascript, to, work, prop...  \n",
      "2  [clipboard, search, history, and, several, oth...  \n",
      "3                    [skip, to, main, page, content]  \n",
      "4  [an, official, website, of, the, united, state...  \n",
      "Les textes tokenisés ont été sauvegardés dans 'contenu_tokenise.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/verdianeouattara/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Télécharger les ressources nécessaires\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Charger le fichier CSV\n",
    "file_path = 'contenu_textuel_nettoye.csv'  # Nom de votre fichier\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Vérifier que la colonne est bien du texte et gérer les valeurs manquantes\n",
    "df['contenu_textuel_nettoye'] = df['contenu_textuel_nettoye'].fillna('').astype(str)\n",
    "\n",
    "# Appliquer word_tokenize pour tokeniser chaque ligne\n",
    "df['tokens'] = df['contenu_textuel_nettoye'].apply(word_tokenize)\n",
    "\n",
    "# Afficher un exemple\n",
    "print(\"Exemple de tokenisation :\")\n",
    "print(df[['contenu_textuel_nettoye', 'tokens']].head())\n",
    "\n",
    "# Sauvegarder les résultats dans un nouveau fichier CSV\n",
    "output_file = 'contenu_tokenise.csv'\n",
    "df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "print(f\"Les textes tokenisés ont été sauvegardés dans '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "— Effectuez une normalisation des mots (racine ou radical)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code pour le stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple de stemming :\n",
      "                                              tokens  \\\n",
      "0  [the, role, of, telomerase, in, breast, cancer...   \n",
      "1  [this, site, needs, javascript, to, work, prop...   \n",
      "2  [clipboard, search, history, and, several, oth...   \n",
      "3                    [skip, to, main, page, content]   \n",
      "4  [an, official, website, of, the, united, state...   \n",
      "\n",
      "                                      stemmed_tokens  \n",
      "0  [the, role, of, telomeras, in, breast, cancer,...  \n",
      "1  [thi, site, need, javascript, to, work, proper...  \n",
      "2  [clipboard, search, histori, and, sever, other...  \n",
      "3                    [skip, to, main, page, content]  \n",
      "4  [an, offici, websit, of, the, unit, state, gov...  \n",
      "Les mots réduits à leur racine ont été sauvegardés dans 'contenu_stemmed.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/verdianeouattara/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Télécharger les ressources nécessaires\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialiser le stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Charger le fichier tokenisé\n",
    "file_path = 'contenu_tokenise.csv'  # Nom du fichier CSV tokenisé\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Assurez-vous que les tokens sont chargés comme des listes\n",
    "df['tokens'] = df['tokens'].apply(eval)  # Convertir les chaînes en listes si nécessaire\n",
    "\n",
    "# Appliquer le stemming sur chaque token\n",
    "df['stemmed_tokens'] = df['tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
    "\n",
    "# Afficher un exemple\n",
    "print(\"Exemple de stemming :\")\n",
    "print(df[['tokens', 'stemmed_tokens']].head())\n",
    "\n",
    "# Sauvegarder dans un fichier CSV\n",
    "output_file = 'contenu_stemmed.csv'\n",
    "df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "print(f\"Les mots réduits à leur racine ont été sauvegardés dans '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/verdianeouattara/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/verdianeouattara/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple de lemmatisation :\n",
      "                                              tokens  \\\n",
      "0  [the, role, of, telomerase, in, breast, cancer...   \n",
      "1  [this, site, needs, javascript, to, work, prop...   \n",
      "2  [clipboard, search, history, and, several, oth...   \n",
      "3                    [skip, to, main, page, content]   \n",
      "4  [an, official, website, of, the, united, state...   \n",
      "\n",
      "                                   lemmatized_tokens  \n",
      "0  [the, role, of, telomerase, in, breast, cancer...  \n",
      "1  [this, site, need, javascript, to, work, prope...  \n",
      "2  [clipboard, search, history, and, several, oth...  \n",
      "3                    [skip, to, main, page, content]  \n",
      "4  [an, official, website, of, the, united, state...  \n",
      "Les mots lemmatisés ont été sauvegardés dans 'contenu_lemmatized.csv'.\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Télécharger les ressources nécessaires pour la lemmatisation\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialiser le lemmatiseur\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Appliquer la lemmatisation sur chaque token\n",
    "df['lemmatized_tokens'] = df['tokens'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
    "\n",
    "# Afficher un exemple\n",
    "print(\"Exemple de lemmatisation :\")\n",
    "print(df[['tokens', 'lemmatized_tokens']].head())\n",
    "\n",
    "# Sauvegarder dans un fichier CSV\n",
    "output_file = 'contenu_lemmatized.csv'\n",
    "df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "print(f\"Les mots lemmatisés ont été sauvegardés dans '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "— Comparez les résultats obtenus entre le stemming et la lemmatisation (La lemmatisation ramène les mots à leur forme canonique en tenant compte du contexte grammatical.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparaison entre tokens, stemming et lemmatisation :\n",
      "Tokens d'origine : ['the', 'role', 'of', 'telomerase', 'in', 'breast', 'cancers', 'response', 'to', 'therapy', 'pubmed']\n",
      "Stemming : ['the', 'role', 'of', 'telomeras', 'in', 'breast', 'cancer', 'respons', 'to', 'therapi', 'pubm']\n",
      "Lemmatisation : ['the', 'role', 'of', 'telomerase', 'in', 'breast', 'cancer', 'response', 'to', 'therapy', 'pubmed']\n",
      "\n",
      "Tokens d'origine : ['this', 'site', 'needs', 'javascript', 'to', 'work', 'properly', 'please', 'enable', 'it', 'to', 'take', 'advantage', 'of', 'the', 'complete', 'set', 'of', 'features']\n",
      "Stemming : ['thi', 'site', 'need', 'javascript', 'to', 'work', 'properli', 'pleas', 'enabl', 'it', 'to', 'take', 'advantag', 'of', 'the', 'complet', 'set', 'of', 'featur']\n",
      "Lemmatisation : ['this', 'site', 'need', 'javascript', 'to', 'work', 'properly', 'please', 'enable', 'it', 'to', 'take', 'advantage', 'of', 'the', 'complete', 'set', 'of', 'feature']\n",
      "\n",
      "Tokens d'origine : ['clipboard', 'search', 'history', 'and', 'several', 'other', 'advanced', 'features', 'are', 'temporarily', 'unavailable']\n",
      "Stemming : ['clipboard', 'search', 'histori', 'and', 'sever', 'other', 'advanc', 'featur', 'are', 'temporarili', 'unavail']\n",
      "Lemmatisation : ['clipboard', 'search', 'history', 'and', 'several', 'other', 'advanced', 'feature', 'are', 'temporarily', 'unavailable']\n",
      "\n",
      "Tokens d'origine : ['skip', 'to', 'main', 'page', 'content']\n",
      "Stemming : ['skip', 'to', 'main', 'page', 'content']\n",
      "Lemmatisation : ['skip', 'to', 'main', 'page', 'content']\n",
      "\n",
      "Tokens d'origine : ['an', 'official', 'website', 'of', 'the', 'united', 'states', 'government']\n",
      "Stemming : ['an', 'offici', 'websit', 'of', 'the', 'unit', 'state', 'govern']\n",
      "Lemmatisation : ['an', 'official', 'website', 'of', 'the', 'united', 'state', 'government']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Comparaison entre tokens, stemming et lemmatisation :\")\n",
    "for _, row in df.head(5).iterrows():\n",
    "    print(f\"Tokens d'origine : {row['tokens']}\")\n",
    "    print(f\"Stemming : {row['stemmed_tokens']}\")\n",
    "    print(f\"Lemmatisation : {row['lemmatized_tokens']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions :\n",
    "\n",
    "\n",
    "— Quelle est la différence entre le stemming et la lemmatisation ?\n",
    "\n",
    "\n",
    "— Quels sont les avantages et les inconvénients de chaque méthode ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarde des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les données brutes, nettoyées, tokenisées, stemmées et lemmatisées ont été sauvegardées dans 'data_resultats_nettoyage.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/verdianeouattara/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/verdianeouattara/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/verdianeouattara/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'contenu_textuel_nettoye.csv'  # Fichier avec les textes nettoyés\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "df['contenu_textuel_nettoye'] = df['contenu_textuel_nettoye'].fillna('').astype(str)  # Gérer les valeurs nulles\n",
    "df['tokens'] = df['contenu_textuel_nettoye'].apply(word_tokenize)\n",
    "\n",
    "# Ajouter une colonne supplémentaire pour les données nettoyées et tokenisées\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialiser stemmer et lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Ajouter stemming\n",
    "df['stemmed_tokens'] = df['tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
    "\n",
    "# Ajouter lemmatisation\n",
    "df['lemmatized_tokens'] = df['tokens'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
    "\n",
    "# Sauvegarder dans un fichier CSV structuré\n",
    "output_file = 'data_resultats_nettoyage.csv'\n",
    "df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Les données brutes, nettoyées, tokenisées, stemmées et lemmatisées ont été sauvegardées dans '{output_file}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
